{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's collect missclassified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import requests\n",
    "from PIL import Image\n",
    "import os\n",
    "import io\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../../skin_cancer_nas\")) # go to parent dir\n",
    "sys.path.append(os.path.abspath(\"/mnt/skin_cancer_nas/data\"))\n",
    "sys.path.append(os.path.abspath(\"/mnt/skin_cancer_nas/data/torch_generator\"))\n",
    "sys.path.append(os.path.abspath(\"/mnt/skin_cancer_nas/nas/darts_torch\"))\n",
    "\n",
    "sys.path.append('/mnt')\n",
    "sys.path.append('/mnt/skin_cancer_nas')\n",
    "sys.path.append('/mnt/skin_cancer_nas/data/torch_generator')\n",
    "from skin_cancer_nas.data.torch_generator import generator as data_gen\n",
    "from skin_cancer_nas.data.torch_generator import base_classes\n",
    "from skin_cancer_nas.data.torch_generator.config import *\n",
    "\n",
    "from nas.darts_torch import *\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "ROOT_PATHS = [  Path('/mnt/data/interim/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels/checkyourskin'),\n",
    "                Path('/mnt/data/interim/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels/loc'),\n",
    "                Path('/mnt/data/interim/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels/loc_old_colored')]\n",
    "\n",
    "CLS1 = ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['c43', 'd03', 'd03.9'], class_name='Melanoma_like_lesions', int_label=0)\n",
    "CLS2 = ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['d22', 'd81', 'l81.2','l81.4', 'q82.5'], class_name='Pigmented_benign', int_label=1)\n",
    "CLS3 = ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['d86.3', 'l21', 'l57', 'l57.0', 'l82', 'l85', 'l85.1', 'l85.5', 'l85.8', 'q80'], class_name='Keratin_lesions', int_label=2)\n",
    "CLS4 = ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['c44', 'c46', 'd09'], class_name='Nonmelanoma_skin_cancer', int_label=3)\n",
    "CLS5 = ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['a63', 'd18', 'd21.9', 'd48', 'l92', 'l94.2', 'l98.8', 'pxe', 'b07', 'ada', 'l57.9', 'l98.9'], class_name='Other', int_label=4)\n",
    "\n",
    "CLASSES_SET_1 = [CLS1, CLS2]\n",
    "CLASSES_SET_2 = [CLS1, CLS2, CLS3]\n",
    "CLASSES_SET_3 = [CLS1, CLS2, CLS3, CLS4]\n",
    "CLASSES_SET_4 = [ \\\n",
    "    ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['c43'], class_name='Melanoma', int_label=0), \\\n",
    "    ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['d22'], class_name='Nevuss', int_label=1) \\\n",
    "]\n",
    "CLASSES_SET_5 = [ \\\n",
    "    ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['c43'], class_name='Melanoma', int_label=0), \\\n",
    "    ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['d22'], class_name='Nevuss', int_label=1), \\\n",
    "    ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['l82'], class_name='Keratin lesion', int_label=2) \\\n",
    "]\n",
    "CLASSES_SET_6 = [ \\\n",
    "   ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['c43'], class_name='Melanoma', int_label=0), \\\n",
    "   ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['c44'], class_name='NonMelanomaCancer', int_label=1), \\\n",
    "   ClassesDefinition(root_folders_list=ROOT_PATHS, diagnoses_names_list=['d22'], class_name='Nevuss', int_label=2) \\\n",
    "]\n",
    "\n",
    "CLASSES_SET_8 = [CLS1, CLS2, CLS3, CLS4, CLS5]\n",
    "\n",
    "VALID_CHANNELS = ['r-r', 'ir-r', 'g-g', 'uv-0-r', 'uv-0-g', 'white-g']\n",
    "\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "VALUE_MISSING = 0\n",
    "MISSING_CH_IMG = np.ones((IMG_HEIGHT, IMG_WIDTH)) * VALUE_MISSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger('skin_cancer_nas__darts_misclassified_img_collection_notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "device = 'cpu'\n",
    "\n",
    "model = torch.load(\"/mnt/models/darts_retrained/6ch_128x128_no_metainfo_registered/3lrs_2oct_ClassSet3_registered/final_model1.pt\")\n",
    "model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_from_path(sample_path, labels):\n",
    "    '''\n",
    "    Generates one sample of data\n",
    "    '''\n",
    "    try:\n",
    "        # logger.info('Sample path={}'.format(sample_path))\n",
    "        x_out = _convert_img_to_array(sample_path)\n",
    "        x_out = x_out.astype('float32')\n",
    "        x_out /= 255\n",
    "\n",
    "        X = torch.as_tensor(x_out)\n",
    "        y = labels[str(sample_path)]\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = torch.tensor(y).to(device)\n",
    "        return X, y\n",
    "    except Exception as e:\n",
    "        msg = 'Something went wrong at path={}, e={}'.format(sample_path, e)\n",
    "        print(msg)\n",
    "        logger.info(msg)\n",
    "\n",
    "\n",
    "def _convert_img_to_array(sample_path):\n",
    "    'Converts n grayscale images to 3D array with n channels'\n",
    "    x_array = []\n",
    "    for channel in VALID_CHANNELS:\n",
    "        channel_path = os.path.join(sample_path, channel)\n",
    "        if not os.path.exists(channel_path):\n",
    "            x_array.append(MISSING_CH_IMG)\n",
    "            continue\n",
    "        image = os.listdir(channel_path)\n",
    "        if not image:\n",
    "            x_array.append(MISSING_CH_IMG)\n",
    "            continue\n",
    "        full_image_path = os.path.join(channel_path, image[0])\n",
    "        img = cv2.imread(full_image_path, flags=cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            x_array.append(MISSING_CH_IMG)\n",
    "            continue\n",
    "        else:\n",
    "            shape = img.shape\n",
    "            # logger.info(shape)\n",
    "            img = cv2.resize(img, (IMG_HEIGHT, IMG_WIDTH), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        x_array.append(img)\n",
    "\n",
    "    return np.stack(x_array, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! IMPORTANT ! - for now we don't have any image transformations (like normalization), but once they will be - they have to be added here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition, labels = data_gen.train_val_split(val_ratio=0.1, classes_list=CLASSES_SET_2)\n",
    "X, y = get_input_output_from_path(sample_path=partition['validation'][0], labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_predictions(partition, partition_key, labels, model):\n",
    "\n",
    "    # Gather list of input 'image' matrices, labels and paths (for titles)\n",
    "    X_list = []\n",
    "    y_list = []  # y_true\n",
    "    y_pred_list = []\n",
    "    scores_list = []\n",
    "    paths_list = []\n",
    "    len_val = len(partition[partition_key])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i in tqdm(range(len_val)):\n",
    "        sample_path = partition[partition_key][i]\n",
    "        _X, _y = get_input_output_from_path(sample_path=sample_path, labels=labels)\n",
    "        scores = model(_X.unsqueeze(0))\n",
    "        scores = scores.detach().numpy()\n",
    "        y_pred = scores.argmax()\n",
    "\n",
    "        X_list.append(_X)\n",
    "        y_list.append(_y.detach().numpy())\n",
    "        paths_list.append(sample_path)\n",
    "        scores_list.append(scores)\n",
    "        y_pred_list.append(y_pred)\n",
    "        \n",
    "    return X_list, y_list, y_pred_list, scores_list, paths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e87551d2434a54b0f42da552177624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_list, y_list, y_pred_list, scores_list, paths_list = gather_predictions(partition, 'validation', labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score 0.8327765674170902\n",
      "precision_score 0.8339285714285715\n",
      "recall_score 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print('f1_score', f1_score(y_list, y_pred_list, average='weighted'))\n",
    "print('precision_score', precision_score(y_list, y_pred_list, average='weighted'))\n",
    "print('recall_score', recall_score(y_list, y_pred_list, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d1cacb82e704cb696e93a43e265949e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=90.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gather missclassified cases\n",
    "subfolders_to_gather = ['r', 'ir', 'g', 'uv-0']\n",
    "channels_to_gather = ['b','g','r']\n",
    "\n",
    "output_folder = '/mnt/data/interim/misclassification_visualization/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels__MISSCLASSIFIED__Set3_Original_9-1_split_validation'\n",
    "\n",
    "for y_true, y_pred, path in tqdm(zip(y_list, y_pred_list, paths_list), total=len(y_list)):\n",
    "    if y_true != y_pred:\n",
    "#         print(y_true, y_pred, path)\n",
    "        for sub_folder in subfolders_to_gather:\n",
    "            if os.path.exists(os.path.join(path, sub_folder + '-' + channels_to_gather[0])):\n",
    "                channels = {}\n",
    "                for ch in channels_to_gather:\n",
    "                    sub_folder_path = os.path.join(path, sub_folder + '-' + ch)\n",
    "                    channel_file = os.listdir(sub_folder_path)[0]\n",
    "                    channels[ch] = cv2.imread(os.path.join(sub_folder_path, channel_file), cv2.IMREAD_GRAYSCALE)\n",
    "                    \n",
    "                color_img = cv2.merge((channels['b'], channels['g'], channels['r']))\n",
    "                color_img_name = path.replace('/mnt/data/interim/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels/', '').replace('/', '---') + sub_folder + '.png'\n",
    "                error_type_folder_name = str(y_true) + \"_classified_as_\" + str(y_pred)\n",
    "                \n",
    "                os.makedirs(os.path.join(output_folder, error_type_folder_name), exist_ok=True)\n",
    "                \n",
    "                color_img_path = os.path.join(output_folder, error_type_folder_name, color_img_name)\n",
    "                cv2.imwrite(color_img_path, color_img)\n",
    "#                 print('Saving file - {}'.format(color_img_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather misclassified images for 5-Fold CV - to get all of them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_misclassified_images(fold_no, y_list, y_pred_list, paths_list):\n",
    "    subfolders_to_gather = ['r', 'ir', 'g', 'uv-0']\n",
    "    channels_to_gather = ['b','g','r']\n",
    "\n",
    "    output_folder = '/mnt/data/interim/misclassification_visualization/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels__MISSCLASSIFIED__5layers_Set3_Original_5FoldCV_foldN{}'.format(fold_no)\n",
    "\n",
    "    for y_true, y_pred, path in tqdm(zip(y_list, y_pred_list, paths_list), total=len(y_list)):\n",
    "        if y_true != y_pred:\n",
    "    #         print(y_true, y_pred, path)\n",
    "            for sub_folder in subfolders_to_gather:\n",
    "                if os.path.exists(os.path.join(path, sub_folder + '-' + channels_to_gather[0])):\n",
    "                    channels = {}\n",
    "                    for ch in channels_to_gather:\n",
    "                        sub_folder_path = os.path.join(path, sub_folder + '-' + ch)\n",
    "                        channel_file = os.listdir(sub_folder_path)[0]\n",
    "                        channels[ch] = cv2.imread(os.path.join(sub_folder_path, channel_file), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                    color_img = cv2.merge((channels['b'], channels['g'], channels['r']))\n",
    "                    color_img_name = path.replace('/mnt/data/interim/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels/', '').replace('/', '---') + sub_folder + '.png'\n",
    "                    error_type_folder_name = str(y_true) + \"_classified_as_\" + str(y_pred)\n",
    "\n",
    "                    os.makedirs(os.path.join(output_folder, error_type_folder_name), exist_ok=True)\n",
    "\n",
    "                    color_img_path = os.path.join(output_folder, error_type_folder_name, color_img_name)\n",
    "                    cv2.imwrite(color_img_path, color_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold_n 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec48cb1b73e45738f0f957e361c5664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.8100215551424639\n",
      "precision_score 0.8127298456115375\n",
      "recall_score 0.8089887640449438\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb129d77a864f08bda768cc5e76f4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold_n 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f371643447524bc3be05e2c8a62b446f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.769737730479169\n",
      "precision_score 0.7718440185062788\n",
      "recall_score 0.7696629213483146\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f3e55ad0eb4c7c9679f8ca0ea6efdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=178.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold_n 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c0ec2af9304543ae8dfa04ce4f816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.7614734410710224\n",
      "precision_score 0.7795940840030361\n",
      "recall_score 0.7670454545454546\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35d7463489f43f4810ef1c1fa4b07bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold_n 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d21d285f1b94d8785b7fbc958339050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=175.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.8047816227940452\n",
      "precision_score 0.8086538605650337\n",
      "recall_score 0.8057142857142857\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8addf543f3014b9aacfb960e2ae6544a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=175.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold_n 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e532e4deabb4f3a99ff3eb51cee7440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=175.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.7723788638262323\n",
      "precision_score 0.7708861748900829\n",
      "recall_score 0.7771428571428571\n",
      "-------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b612332cee02423ca07f4eb282e973bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=175.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/interim/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels/checkyourskin/L82/2019-01-25_11-39-37/uv-0-r'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-4ee461119537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mwrite_misclassified_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-5d380cc7c59d>\u001b[0m in \u001b[0;36mwrite_misclassified_images\u001b[0;34m(fold_no, y_list, y_pred_list, paths_list)\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchannels_to_gather\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                         \u001b[0msub_folder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                         \u001b[0mchannel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_folder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                         \u001b[0mchannels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_folder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/interim/_melanoma_20200728_REGISTERED_OCV_WHITE_Split_Channels/checkyourskin/L82/2019-01-25_11-39-37/uv-0-r'"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from base_classes import Dataset\n",
    "\n",
    "class RandomRot(object):\n",
    "    \n",
    "    '''\n",
    "    Randomly rotates +90 / -90 degrees (or not rotates) passed in 3D tensor\n",
    "    '''\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        :param sample: torch tensor \n",
    "\n",
    "        :return: randomly rotated for 90, 180, 270 or 0 times\n",
    "        \"\"\"\n",
    "        k = random.randint(0, 2) - 1\n",
    "        if k != 0:\n",
    "            sample = torch.rot90(sample, k, [1, 2])\n",
    "        return sample\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \n",
    "    def __init__(self, horizontal=True, prob_threshold=0.5):\n",
    "        self.prob_threshold = prob_threshold\n",
    "        self.horizontal = horizontal\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        :param sample: torch tensor \n",
    "\n",
    "        :return: randomly horizontally/vertically flipped torch 3D tensor\n",
    "        \"\"\"\n",
    "        flip_prob = random.uniform(0, 1)\n",
    "        if flip_prob > self.prob_threshold:\n",
    "            if self.horizontal:\n",
    "                sample = torch.flip(sample, [2])  # vertical flip of 3D tensor\n",
    "            else:\n",
    "                sample = torch.flip(sample, [1]) # horizontal flip of 3D tensor\n",
    "        return sample\n",
    "\n",
    "def prepare_train_val_generators(partition, labels):\n",
    "    # MEAN = [0.2336, 0.6011, 0.3576, 0.4543]\n",
    "    # STD = [0.0530, 0.0998, 0.0965, 0.1170]\n",
    "    normalize = [\n",
    "        # # transforms.Normalize(MEAN, STD)\n",
    "        # transforms.ToPILImage(),\n",
    "        # transforms.RandomHorizontalFlip(p=0.5),\n",
    "        # transforms.RandomRotation(degrees=(-90, 90)),\n",
    "        # transforms.RandomVerticalFlip(p=0.5),\n",
    "        # transforms.ToTensor(),\n",
    "        RandomFlip(horizontal=True, prob_threshold=0.5),\n",
    "        RandomRot(),\n",
    "        RandomFlip(horizontal=False, prob_threshold=0.5)\n",
    "    ]\n",
    "    train_transform = transforms.Compose(normalize)\n",
    "    valid_transform = transforms.Compose([])\n",
    "\n",
    "    # Generators Declaration\n",
    "    data_device = torch.device(\"cpu\")\n",
    "    # data_device = device\n",
    "\n",
    "    def _init_fn(worker_id):\n",
    "        np.random.seed(int(seed))\n",
    "\n",
    "    training_set = Dataset( partition['train'], \n",
    "                            labels, \n",
    "                            transform=train_transform, \n",
    "                            device=data_device,\n",
    "                            valid_channels=VALID_CHANNELS,\n",
    "                            channels_to_zero_out=[])\n",
    "    train_loader = torch.utils.data.DataLoader( training_set, \n",
    "                                                **data_gen.PARAMS, \n",
    "                                                pin_memory=True, \n",
    "                                                worker_init_fn=_init_fn)\n",
    "\n",
    "    validation_set = Dataset(partition['validation'], \n",
    "                            labels, \n",
    "                            transform=valid_transform, \n",
    "                            device=data_device,\n",
    "                            valid_channels=VALID_CHANNELS,\n",
    "                            channels_to_zero_out=[])\n",
    "    valid_loader = torch.utils.data.DataLoader( validation_set, \n",
    "                                                **data_gen.PARAMS, \n",
    "                                                pin_memory=True, \n",
    "                                                worker_init_fn=_init_fn)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "\n",
    "folds_partions_dict, labels = data_gen.train_val_split_kfolds(folds_n=5, classes_list=CLASSES_SET_3)\n",
    "\n",
    "for fold_n, partition in folds_partions_dict.items():\n",
    "    print('fold_n',fold_n)\n",
    "    \n",
    "    train_loader, valid_loader = prepare_train_val_generators(partition, labels)\n",
    "    \n",
    "    model = torch.load(\"/mnt/models/darts_retrained/6ch_128x128_no_metainfo_registered_5Fold/5lrs_2oct_ClassSet3_registered_fold-{}/final_model1.pt\".format(fold_n))\n",
    "    model.to(device)\n",
    "    \n",
    "    X_list, y_list, y_pred_list, scores_list, paths_list = gather_predictions(partition, 'validation', labels, model)\n",
    "    \n",
    "    print('f1_score', f1_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('precision_score', precision_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('recall_score', recall_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('-------------------')\n",
    "    \n",
    "    write_misclassified_images(fold_n, y_list, y_pred_list, paths_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect estimations for randomly dropped channels original 3 layers Set3 network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition, labels = data_gen.train_val_split(val_ratio=0.1, classes_list=CLASSES_SET_8)\n",
    "X, y = get_input_output_from_path(sample_path=partition['validation'][0], labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbd84b1cff34606824ee3c52085a754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1249.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.7906912632362361\n",
      "precision_score 0.8138043505146004\n",
      "recall_score 0.7934347477982386\n",
      "confusion_matrix \n",
      " [[ 29   3   0   2   8]\n",
      " [  5 305   8   1  50]\n",
      " [  1  18 147   3  57]\n",
      " [  3   2   2  92  56]\n",
      " [  0  32   3   4 418]]\n",
      "Layers 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec32e284235495ab177318c32e55a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1249.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.9525632153904127\n",
      "precision_score 0.9531127398079756\n",
      "recall_score 0.9527622097678142\n",
      "confusion_matrix \n",
      " [[ 41   0   0   1   0]\n",
      " [  1 356   4   1   7]\n",
      " [  0   7 203   5  11]\n",
      " [  2   0   1 144   8]\n",
      " [  0   8   1   2 446]]\n",
      "Layers 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a20ed759c64467bb3e4a2a4bb1cee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1249.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.9316971649951324\n",
      "precision_score 0.9334437088264412\n",
      "recall_score 0.9319455564451561\n",
      "confusion_matrix \n",
      " [[ 35   1   0   2   4]\n",
      " [  0 352   6   1  10]\n",
      " [  0   5 198   7  16]\n",
      " [  0   0   0 139  16]\n",
      " [  0  14   1   2 440]]\n",
      "Layers 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeedafb0ff834958939dc5ce63f9712f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1249.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.9493151079532723\n",
      "precision_score 0.9499189548308733\n",
      "recall_score 0.9495596477181746\n",
      "confusion_matrix \n",
      " [[ 41   0   0   0   1]\n",
      " [  0 363   3   1   2]\n",
      " [  0   5 207   5   9]\n",
      " [  2   0   1 138  14]\n",
      " [  0  17   2   1 437]]\n",
      "Layers 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28679b50068a4eaa86a5f65f80ae573b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1249.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.9590270264667781\n",
      "precision_score 0.9599253017333931\n",
      "recall_score 0.9591673338670936\n",
      "confusion_matrix \n",
      " [[ 42   0   0   0   0]\n",
      " [  0 361   4   0   4]\n",
      " [  0   8 211   3   4]\n",
      " [  4   0   0 139  12]\n",
      " [  1  11   0   0 445]]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = 'cpu'\n",
    "\n",
    "for L in range(1,6):\n",
    "    print('Layers {}'.format(L))\n",
    "    model = torch.load('/mnt/models/darts_retrained/6ch_128x128_no_metainfo_registered/XV2_SGD_orig_02DropChannel_{}lrs_2oct_ClassSet8_ManCorected_registered/final_model1.pt'.format(L))\n",
    "    model.to(device)\n",
    "\n",
    "    X_list, y_list, y_pred_list, scores_list, paths_list = gather_predictions(partition, 'train', labels, model)  #validation\n",
    "\n",
    "    print('f1_score', f1_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('precision_score', precision_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('recall_score', recall_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('confusion_matrix \\n', confusion_matrix(y_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0aa5f81c6b44899d25119438412c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=141.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.6784275753572818\n",
      "precision_score 0.726462965925452\n",
      "recall_score 0.6879432624113475\n",
      "confusion_matrix \n",
      " [[ 4  1  0  0  0]\n",
      " [ 0 30  0  1 10]\n",
      " [ 0  2 13  1 10]\n",
      " [ 1  0  0  6 11]\n",
      " [ 0  5  1  1 44]]\n",
      "Layers 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e327c11c944126910efed353286d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=141.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.7708284415928545\n",
      "precision_score 0.7773271454327264\n",
      "recall_score 0.7730496453900709\n",
      "confusion_matrix \n",
      " [[ 4  1  0  0  0]\n",
      " [ 0 35  0  1  5]\n",
      " [ 0  1 17  2  6]\n",
      " [ 1  0  1 11  5]\n",
      " [ 0  6  2  1 42]]\n",
      "Layers 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f41ef0ee79704fa3b6a28176ae9af089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=141.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.7458565671237485\n",
      "precision_score 0.757841166180219\n",
      "recall_score 0.7446808510638298\n",
      "confusion_matrix \n",
      " [[ 4  1  0  0  0]\n",
      " [ 1 31  0  0  9]\n",
      " [ 0  1 17  0  8]\n",
      " [ 1  0  1 12  4]\n",
      " [ 0  3  4  3 41]]\n",
      "Layers 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4930d4cdd0714cbbb99a187af192bda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=141.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.7933521314760108\n",
      "precision_score 0.7978948850918174\n",
      "recall_score 0.7943262411347518\n",
      "confusion_matrix \n",
      " [[ 4  1  0  0  0]\n",
      " [ 1 36  0  0  4]\n",
      " [ 0  1 19  2  4]\n",
      " [ 1  0  1 11  5]\n",
      " [ 0  5  1  3 42]]\n",
      "Layers 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00528497771745399c266147793f44c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=141.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "f1_score 0.7681348030216923\n",
      "precision_score 0.7757245709373369\n",
      "recall_score 0.7730496453900709\n",
      "confusion_matrix \n",
      " [[ 4  1  0  0  0]\n",
      " [ 1 37  1  0  2]\n",
      " [ 0  1 18  0  7]\n",
      " [ 1  0  2  9  6]\n",
      " [ 0  5  3  2 41]]\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device = 'cpu'\n",
    "\n",
    "for L in range(1,6):\n",
    "    print('Layers {}'.format(L))\n",
    "    model = torch.load('/mnt/models/darts_retrained/6ch_128x128_no_metainfo_registered/XV2_SGD_orig_02DropChannel_{}lrs_2oct_ClassSet8_ManCorected_registered/final_model1.pt'.format(L))\n",
    "    model.to(device)\n",
    "\n",
    "    X_list, y_list, y_pred_list, scores_list, paths_list = gather_predictions(partition, 'validation', labels, model)  #validation | train\n",
    "\n",
    "    print('f1_score', f1_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('precision_score', precision_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('recall_score', recall_score(y_list, y_pred_list, average='weighted'))\n",
    "    print('confusion_matrix \\n', confusion_matrix(y_list, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
